{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16 Model on CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to apply the VG-16 pretrained model on the CIFAR-10 dataset. \n",
    "\n",
    "* **About VGG-16**\n",
    "The VGG model is based on the Very Deep Convolutional Networks for Large-Scale Image Recognition paper. VGG16 is a convolution neural net architecture used for image recognition. It uses 16 layers with weights. https://arxiv.org/abs/1409.1556\n",
    "\n",
    "* **About CIFAR-10**\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import packages and notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requisite imports\n",
    "import torch\n",
    "import tqdm \n",
    "import time\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device setting**: Devices with MPS available will use MPS, cuda will use cuda, and if both are not available, `device` is set to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "# Check if mps is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "# check if cuda is available\n",
    "elif torch.backends.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seed Setting**:\n",
    "Different sources of randomness contribute to the result of a neural network model. Nevertheless, a good neural network model should not depend on the eed but the data, architecture, and hyperparameters used. We introduce a seed value for the sake of reproducibility of our results. We set the `seed_value` to `42` for the following sources of randomness:\n",
    "1. within the environment\n",
    "2. within Python\n",
    "3. within some packages like numpy and torch\n",
    "4. and anywhere else where randomness is introduced like within architectures (some dropout layers introduce randomness) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1195466b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` and `torch` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) Loading CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:06<00:00, 27249939.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(\"data/\", train=True, download=True) \n",
    "# val_size = 10000 \n",
    "# train_size = len(train) - val_size\n",
    "# train, val = random_split(train, [train_size, val_size]) \n",
    "test = torchvision.datasets.CIFAR10(\"data/\", train=False, download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
