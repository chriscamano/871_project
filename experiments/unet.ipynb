{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Model on CIFAR-10 Dataset\n",
    "*Author: Preetham Ramesh*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The goal of this notebook is to apply the UNet pretrained model on the CIFAR-10 dataset.\n",
    "\n",
    "## Resources\n",
    "* **About UNet**\n",
    "The UNet model is a convolutional neural network commonly used for image segmentation tasks. Its architecture consists of a contracting path to capture context and a symmetric expanding path for precise localization. This U-shaped network structure enables the model to efficiently learn and segment images by combining high-resolution features from the contracting path with localization information from the expanding path.\n",
    "\n",
    "* **About CIFAR-10**\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. http://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Import packages and notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else ('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seed Setting**:\n",
    "Different sources of randomness contribute to the result of a neural network model. Nevertheless, a good neural network model should not depend on the eed but the data, architecture, and hyperparameters used. We introduce a seed value for the sake of reproducibility of our results. We set the `seed_value` to `42` for the following sources of randomness:\n",
    "1. within the environment\n",
    "2. within Python\n",
    "3. within some packages like numpy and torch\n",
    "4. and anywhere else where randomness is introduced like within architectures (some dropout layers introduce randomness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce7410f2f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` and `torch` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class UNetCIFAR(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=10, init_features=32):\n",
    "        super(UNetCIFAR, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = UNetCIFAR._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNetCIFAR._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetCIFAR._block(features * 2, features * 4, name=\"bottleneck\")\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNetCIFAR._block(features * 4, features * 2, name=\"dec1\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNetCIFAR._block(features * 2, features, name=\"dec2\")\n",
    "        \n",
    "        # Output Convolution\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        dec1 = self.upconv1(bottleneck)\n",
    "        dec1 = torch.cat((dec1, enc2), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        dec2 = self.upconv2(dec1)\n",
    "        dec2 = torch.cat((dec2, enc1), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        return self.conv(dec2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=features,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform and Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define transformations for the dataset (optional but recommended)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image data\n",
    "])\n",
    "\n",
    "# Download and load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "val_size = 7500\n",
    "\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders to handle batches and shuffling\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Loss: 19.7130\n",
      "Epoch: 1, Batch: 200, Loss: 10.1696\n",
      "Epoch: 1, Batch: 300, Loss: 7.5505\n",
      "Epoch: 1, Batch: 400, Loss: 7.4835\n",
      "Epoch: 1, Batch: 500, Loss: 6.8185\n",
      "Epoch: 1, Batch: 600, Loss: 6.6497\n",
      "Epoch: 2, Batch: 100, Loss: 6.1528\n",
      "Epoch: 2, Batch: 200, Loss: 5.7138\n",
      "Epoch: 2, Batch: 300, Loss: 5.8116\n",
      "Epoch: 2, Batch: 400, Loss: 5.4837\n",
      "Epoch: 2, Batch: 500, Loss: 5.3434\n",
      "Epoch: 2, Batch: 600, Loss: 5.3246\n",
      "Epoch: 3, Batch: 100, Loss: 4.9010\n",
      "Epoch: 3, Batch: 200, Loss: 4.9260\n",
      "Epoch: 3, Batch: 300, Loss: 4.7508\n",
      "Epoch: 3, Batch: 400, Loss: 4.5863\n",
      "Epoch: 3, Batch: 500, Loss: 4.6473\n",
      "Epoch: 3, Batch: 600, Loss: 4.5947\n",
      "Epoch: 4, Batch: 100, Loss: 4.3219\n",
      "Epoch: 4, Batch: 200, Loss: 4.0834\n",
      "Epoch: 4, Batch: 300, Loss: 4.1170\n",
      "Epoch: 4, Batch: 400, Loss: 4.2058\n",
      "Epoch: 4, Batch: 500, Loss: 4.3716\n",
      "Epoch: 4, Batch: 600, Loss: 4.2756\n",
      "Epoch: 5, Batch: 100, Loss: 4.0920\n",
      "Epoch: 5, Batch: 200, Loss: 3.9160\n",
      "Epoch: 5, Batch: 300, Loss: 3.9722\n",
      "Epoch: 5, Batch: 400, Loss: 3.8883\n",
      "Epoch: 5, Batch: 500, Loss: 3.8090\n",
      "Epoch: 5, Batch: 600, Loss: 3.8180\n",
      "Epoch: 6, Batch: 100, Loss: 3.6014\n",
      "Epoch: 6, Batch: 200, Loss: 3.4903\n",
      "Epoch: 6, Batch: 300, Loss: 3.5500\n",
      "Epoch: 6, Batch: 400, Loss: 3.4754\n",
      "Epoch: 6, Batch: 500, Loss: 3.7254\n",
      "Epoch: 6, Batch: 600, Loss: 3.6245\n",
      "Epoch: 7, Batch: 100, Loss: 3.4794\n",
      "Epoch: 7, Batch: 200, Loss: 3.4709\n",
      "Epoch: 7, Batch: 300, Loss: 3.3408\n",
      "Epoch: 7, Batch: 400, Loss: 3.2461\n",
      "Epoch: 7, Batch: 500, Loss: 3.1978\n",
      "Epoch: 7, Batch: 600, Loss: 3.2790\n",
      "Epoch: 8, Batch: 100, Loss: 2.9745\n",
      "Epoch: 8, Batch: 200, Loss: 3.0656\n",
      "Epoch: 8, Batch: 300, Loss: 3.0497\n",
      "Epoch: 8, Batch: 400, Loss: 3.1547\n",
      "Epoch: 8, Batch: 500, Loss: 3.1888\n",
      "Epoch: 8, Batch: 600, Loss: 3.1430\n",
      "Epoch: 9, Batch: 100, Loss: 2.9392\n",
      "Epoch: 9, Batch: 200, Loss: 2.6860\n",
      "Epoch: 9, Batch: 300, Loss: 2.8311\n",
      "Epoch: 9, Batch: 400, Loss: 3.0097\n",
      "Epoch: 9, Batch: 500, Loss: 2.9355\n",
      "Epoch: 9, Batch: 600, Loss: 2.8869\n",
      "Epoch: 10, Batch: 100, Loss: 2.6127\n",
      "Epoch: 10, Batch: 200, Loss: 2.7063\n",
      "Epoch: 10, Batch: 300, Loss: 2.6176\n",
      "Epoch: 10, Batch: 400, Loss: 2.7178\n",
      "Epoch: 10, Batch: 500, Loss: 2.8126\n",
      "Epoch: 10, Batch: 600, Loss: 2.7326\n",
      "Epoch: 11, Batch: 100, Loss: 2.4151\n",
      "Epoch: 11, Batch: 200, Loss: 2.5633\n",
      "Epoch: 11, Batch: 300, Loss: 2.4848\n",
      "Epoch: 11, Batch: 400, Loss: 2.5408\n",
      "Epoch: 11, Batch: 500, Loss: 2.6277\n",
      "Epoch: 11, Batch: 600, Loss: 2.6155\n",
      "Epoch: 12, Batch: 100, Loss: 2.3405\n",
      "Epoch: 12, Batch: 200, Loss: 2.4610\n",
      "Epoch: 12, Batch: 300, Loss: 2.3405\n",
      "Epoch: 12, Batch: 400, Loss: 2.2103\n",
      "Epoch: 12, Batch: 500, Loss: 2.3876\n",
      "Epoch: 12, Batch: 600, Loss: 2.4419\n",
      "Epoch: 13, Batch: 100, Loss: 2.1756\n",
      "Epoch: 13, Batch: 200, Loss: 2.1530\n",
      "Epoch: 13, Batch: 300, Loss: 2.1955\n",
      "Epoch: 13, Batch: 400, Loss: 2.2337\n",
      "Epoch: 13, Batch: 500, Loss: 2.2268\n",
      "Epoch: 13, Batch: 600, Loss: 2.3368\n",
      "Epoch: 14, Batch: 100, Loss: 1.9079\n",
      "Epoch: 14, Batch: 200, Loss: 1.9855\n",
      "Epoch: 14, Batch: 300, Loss: 2.1032\n",
      "Epoch: 14, Batch: 400, Loss: 2.0800\n",
      "Epoch: 14, Batch: 500, Loss: 2.0682\n",
      "Epoch: 14, Batch: 600, Loss: 2.1232\n",
      "Epoch: 15, Batch: 100, Loss: 1.8910\n",
      "Epoch: 15, Batch: 200, Loss: 1.9488\n",
      "Epoch: 15, Batch: 300, Loss: 1.9945\n",
      "Epoch: 15, Batch: 400, Loss: 1.9845\n",
      "Epoch: 15, Batch: 500, Loss: 2.0047\n",
      "Epoch: 15, Batch: 600, Loss: 1.9748\n",
      "Epoch: 16, Batch: 100, Loss: 2.0090\n",
      "Epoch: 16, Batch: 200, Loss: 1.8250\n",
      "Epoch: 16, Batch: 300, Loss: 1.7915\n",
      "Epoch: 16, Batch: 400, Loss: 1.8176\n",
      "Epoch: 16, Batch: 500, Loss: 1.8297\n",
      "Epoch: 16, Batch: 600, Loss: 1.8418\n",
      "Epoch: 17, Batch: 100, Loss: 1.6725\n",
      "Epoch: 17, Batch: 200, Loss: 1.7073\n",
      "Epoch: 17, Batch: 300, Loss: 1.7197\n",
      "Epoch: 17, Batch: 400, Loss: 1.7258\n",
      "Epoch: 17, Batch: 500, Loss: 1.7095\n",
      "Epoch: 17, Batch: 600, Loss: 1.7709\n",
      "Epoch: 18, Batch: 100, Loss: 1.9274\n",
      "Epoch: 18, Batch: 200, Loss: 1.6382\n",
      "Epoch: 18, Batch: 300, Loss: 1.5875\n",
      "Epoch: 18, Batch: 400, Loss: 1.6223\n",
      "Epoch: 18, Batch: 500, Loss: 1.6563\n",
      "Epoch: 18, Batch: 600, Loss: 1.7335\n",
      "Epoch: 19, Batch: 100, Loss: 2.2366\n",
      "Epoch: 19, Batch: 200, Loss: 1.6995\n",
      "Epoch: 19, Batch: 300, Loss: 1.6167\n",
      "Epoch: 19, Batch: 400, Loss: 1.5823\n",
      "Epoch: 19, Batch: 500, Loss: 1.5954\n",
      "Epoch: 19, Batch: 600, Loss: 1.4832\n",
      "Epoch: 20, Batch: 100, Loss: 1.3712\n",
      "Epoch: 20, Batch: 200, Loss: 1.3864\n",
      "Epoch: 20, Batch: 300, Loss: 1.5288\n",
      "Epoch: 20, Batch: 400, Loss: 1.5721\n",
      "Epoch: 20, Batch: 500, Loss: 1.4876\n",
      "Epoch: 20, Batch: 600, Loss: 1.4363\n",
      "Epoch: 21, Batch: 100, Loss: 1.3687\n",
      "Epoch: 21, Batch: 200, Loss: 1.2948\n",
      "Epoch: 21, Batch: 300, Loss: 1.3327\n",
      "Epoch: 21, Batch: 400, Loss: 1.4076\n",
      "Epoch: 21, Batch: 500, Loss: 1.5026\n",
      "Epoch: 21, Batch: 600, Loss: 1.4405\n",
      "Epoch: 22, Batch: 100, Loss: 1.3639\n",
      "Epoch: 22, Batch: 200, Loss: 1.3598\n",
      "Epoch: 22, Batch: 300, Loss: 1.4364\n",
      "Epoch: 22, Batch: 400, Loss: 1.3353\n",
      "Epoch: 22, Batch: 500, Loss: 1.3884\n",
      "Epoch: 22, Batch: 600, Loss: 1.3813\n",
      "Epoch: 23, Batch: 100, Loss: 1.4097\n",
      "Epoch: 23, Batch: 200, Loss: 1.2264\n",
      "Epoch: 23, Batch: 300, Loss: 1.2223\n",
      "Epoch: 23, Batch: 400, Loss: 1.2023\n",
      "Epoch: 23, Batch: 500, Loss: 1.2383\n",
      "Epoch: 23, Batch: 600, Loss: 1.2616\n",
      "Epoch: 24, Batch: 100, Loss: 2.0565\n",
      "Epoch: 24, Batch: 200, Loss: 1.3955\n",
      "Epoch: 24, Batch: 300, Loss: 1.3057\n",
      "Epoch: 24, Batch: 400, Loss: 1.2640\n",
      "Epoch: 24, Batch: 500, Loss: 1.2673\n",
      "Epoch: 24, Batch: 600, Loss: 1.2257\n",
      "Epoch: 25, Batch: 100, Loss: 1.1290\n",
      "Epoch: 25, Batch: 200, Loss: 1.0828\n",
      "Epoch: 25, Batch: 300, Loss: 1.0947\n",
      "Epoch: 25, Batch: 400, Loss: 1.1781\n",
      "Epoch: 25, Batch: 500, Loss: 1.1404\n",
      "Epoch: 25, Batch: 600, Loss: 1.1306\n",
      "Epoch: 26, Batch: 100, Loss: 1.0526\n",
      "Epoch: 26, Batch: 200, Loss: 1.0353\n",
      "Epoch: 26, Batch: 300, Loss: 1.0095\n",
      "Epoch: 26, Batch: 400, Loss: 1.0378\n",
      "Epoch: 26, Batch: 500, Loss: 1.2654\n",
      "Epoch: 26, Batch: 600, Loss: 1.2307\n",
      "Epoch: 27, Batch: 100, Loss: 1.2299\n",
      "Epoch: 27, Batch: 200, Loss: 1.0360\n",
      "Epoch: 27, Batch: 300, Loss: 1.0247\n",
      "Epoch: 27, Batch: 400, Loss: 1.0648\n",
      "Epoch: 27, Batch: 500, Loss: 1.0368\n",
      "Epoch: 27, Batch: 600, Loss: 1.0504\n",
      "Epoch: 28, Batch: 100, Loss: 1.2173\n",
      "Epoch: 28, Batch: 200, Loss: 1.0818\n",
      "Epoch: 28, Batch: 300, Loss: 0.9528\n",
      "Epoch: 28, Batch: 400, Loss: 1.0076\n",
      "Epoch: 28, Batch: 500, Loss: 1.0288\n",
      "Epoch: 28, Batch: 600, Loss: 1.0467\n",
      "Epoch: 29, Batch: 100, Loss: 1.1829\n",
      "Epoch: 29, Batch: 200, Loss: 0.9732\n",
      "Epoch: 29, Batch: 300, Loss: 0.9395\n",
      "Epoch: 29, Batch: 400, Loss: 0.9345\n",
      "Epoch: 29, Batch: 500, Loss: 1.0784\n",
      "Epoch: 29, Batch: 600, Loss: 1.0533\n",
      "Epoch: 30, Batch: 100, Loss: 0.9975\n",
      "Epoch: 30, Batch: 200, Loss: 0.8673\n",
      "Epoch: 30, Batch: 300, Loss: 0.8887\n",
      "Epoch: 30, Batch: 400, Loss: 0.8745\n",
      "Epoch: 30, Batch: 500, Loss: 0.9001\n",
      "Epoch: 30, Batch: 600, Loss: 0.9560\n",
      "Epoch: 31, Batch: 100, Loss: 0.9953\n",
      "Epoch: 31, Batch: 200, Loss: 0.9031\n",
      "Epoch: 31, Batch: 300, Loss: 0.8699\n",
      "Epoch: 31, Batch: 400, Loss: 0.8680\n",
      "Epoch: 31, Batch: 500, Loss: 0.9152\n",
      "Epoch: 31, Batch: 600, Loss: 0.9178\n",
      "Epoch: 32, Batch: 100, Loss: 1.1697\n",
      "Epoch: 32, Batch: 200, Loss: 0.9026\n",
      "Epoch: 32, Batch: 300, Loss: 0.8267\n",
      "Epoch: 32, Batch: 400, Loss: 0.9310\n",
      "Epoch: 32, Batch: 500, Loss: 1.0513\n",
      "Epoch: 32, Batch: 600, Loss: 1.1407\n",
      "Epoch: 33, Batch: 100, Loss: 1.3029\n",
      "Epoch: 33, Batch: 200, Loss: 0.9580\n",
      "Epoch: 33, Batch: 300, Loss: 0.8777\n",
      "Epoch: 33, Batch: 400, Loss: 0.8813\n",
      "Epoch: 33, Batch: 500, Loss: 0.8631\n",
      "Epoch: 33, Batch: 600, Loss: 0.8622\n",
      "Epoch: 34, Batch: 100, Loss: 0.9026\n",
      "Epoch: 34, Batch: 200, Loss: 0.7831\n",
      "Epoch: 34, Batch: 300, Loss: 0.8176\n",
      "Epoch: 34, Batch: 400, Loss: 0.8128\n",
      "Epoch: 34, Batch: 500, Loss: 0.7869\n",
      "Epoch: 34, Batch: 600, Loss: 0.8250\n",
      "Epoch: 35, Batch: 100, Loss: 0.8576\n",
      "Epoch: 35, Batch: 200, Loss: 0.7627\n",
      "Epoch: 35, Batch: 300, Loss: 0.7281\n",
      "Epoch: 35, Batch: 400, Loss: 0.7763\n",
      "Epoch: 35, Batch: 500, Loss: 0.8232\n",
      "Epoch: 35, Batch: 600, Loss: 0.8451\n",
      "Epoch: 36, Batch: 100, Loss: 0.7933\n",
      "Epoch: 36, Batch: 200, Loss: 0.7407\n",
      "Epoch: 36, Batch: 300, Loss: 0.6967\n",
      "Epoch: 36, Batch: 400, Loss: 0.7213\n",
      "Epoch: 36, Batch: 500, Loss: 0.7833\n",
      "Epoch: 36, Batch: 600, Loss: 0.8350\n",
      "Epoch: 37, Batch: 100, Loss: 0.6790\n",
      "Epoch: 37, Batch: 200, Loss: 0.7144\n",
      "Epoch: 37, Batch: 300, Loss: 0.7762\n",
      "Epoch: 37, Batch: 400, Loss: 0.8175\n",
      "Epoch: 37, Batch: 500, Loss: 0.7800\n",
      "Epoch: 37, Batch: 600, Loss: 0.7531\n",
      "Epoch: 38, Batch: 100, Loss: 1.0297\n",
      "Epoch: 38, Batch: 200, Loss: 0.8879\n",
      "Epoch: 38, Batch: 300, Loss: 0.8229\n",
      "Epoch: 38, Batch: 400, Loss: 0.7235\n",
      "Epoch: 38, Batch: 500, Loss: 0.7027\n",
      "Epoch: 38, Batch: 600, Loss: 0.6884\n",
      "Epoch: 39, Batch: 100, Loss: 0.6135\n",
      "Epoch: 39, Batch: 200, Loss: 0.6298\n",
      "Epoch: 39, Batch: 300, Loss: 0.7047\n",
      "Epoch: 39, Batch: 400, Loss: 0.6808\n",
      "Epoch: 39, Batch: 500, Loss: 0.6780\n",
      "Epoch: 39, Batch: 600, Loss: 0.7815\n",
      "Epoch: 40, Batch: 100, Loss: 1.3213\n",
      "Epoch: 40, Batch: 200, Loss: 0.9351\n",
      "Epoch: 40, Batch: 300, Loss: 0.7535\n",
      "Epoch: 40, Batch: 400, Loss: 0.6767\n",
      "Epoch: 40, Batch: 500, Loss: 0.6852\n",
      "Epoch: 40, Batch: 600, Loss: 0.6920\n",
      "Epoch: 41, Batch: 100, Loss: 0.6033\n",
      "Epoch: 41, Batch: 200, Loss: 0.5654\n",
      "Epoch: 41, Batch: 300, Loss: 0.5943\n",
      "Epoch: 41, Batch: 400, Loss: 0.6151\n",
      "Epoch: 41, Batch: 500, Loss: 0.6240\n",
      "Epoch: 41, Batch: 600, Loss: 0.6303\n",
      "Epoch: 42, Batch: 100, Loss: 0.6324\n",
      "Epoch: 42, Batch: 200, Loss: 0.5783\n",
      "Epoch: 42, Batch: 300, Loss: 0.5620\n",
      "Epoch: 42, Batch: 400, Loss: 0.6303\n",
      "Epoch: 42, Batch: 500, Loss: 0.6380\n",
      "Epoch: 42, Batch: 600, Loss: 0.6695\n",
      "Epoch: 43, Batch: 100, Loss: 0.6038\n",
      "Epoch: 43, Batch: 200, Loss: 0.5954\n",
      "Epoch: 43, Batch: 300, Loss: 0.6023\n",
      "Epoch: 43, Batch: 400, Loss: 0.9058\n",
      "Epoch: 43, Batch: 500, Loss: 0.7468\n",
      "Epoch: 43, Batch: 600, Loss: 0.7056\n",
      "Epoch: 44, Batch: 100, Loss: 0.8808\n",
      "Epoch: 44, Batch: 200, Loss: 0.7023\n",
      "Epoch: 44, Batch: 300, Loss: 0.6565\n",
      "Epoch: 44, Batch: 400, Loss: 0.6064\n",
      "Epoch: 44, Batch: 500, Loss: 0.6119\n",
      "Epoch: 44, Batch: 600, Loss: 0.6080\n",
      "Epoch: 45, Batch: 100, Loss: 0.5334\n",
      "Epoch: 45, Batch: 200, Loss: 0.5218\n",
      "Epoch: 45, Batch: 300, Loss: 0.4998\n",
      "Epoch: 45, Batch: 400, Loss: 0.5291\n",
      "Epoch: 45, Batch: 500, Loss: 0.5642\n",
      "Epoch: 45, Batch: 600, Loss: 0.5651\n",
      "Epoch: 46, Batch: 100, Loss: 0.4928\n",
      "Epoch: 46, Batch: 200, Loss: 0.4913\n",
      "Epoch: 46, Batch: 300, Loss: 0.5227\n",
      "Epoch: 46, Batch: 400, Loss: 0.5346\n",
      "Epoch: 46, Batch: 500, Loss: 0.5283\n",
      "Epoch: 46, Batch: 600, Loss: 0.5524\n",
      "Epoch: 47, Batch: 100, Loss: 0.6021\n",
      "Epoch: 47, Batch: 200, Loss: 0.5836\n",
      "Epoch: 47, Batch: 300, Loss: 0.6686\n",
      "Epoch: 47, Batch: 400, Loss: 0.6746\n",
      "Epoch: 47, Batch: 500, Loss: 0.6555\n",
      "Epoch: 47, Batch: 600, Loss: 0.6351\n",
      "Epoch: 48, Batch: 100, Loss: 0.5416\n",
      "Epoch: 48, Batch: 200, Loss: 0.5101\n",
      "Epoch: 48, Batch: 300, Loss: 0.5381\n",
      "Epoch: 48, Batch: 400, Loss: 0.5261\n",
      "Epoch: 48, Batch: 500, Loss: 0.5587\n",
      "Epoch: 48, Batch: 600, Loss: 0.5996\n",
      "Epoch: 49, Batch: 100, Loss: 0.4995\n",
      "Epoch: 49, Batch: 200, Loss: 0.4811\n",
      "Epoch: 49, Batch: 300, Loss: 0.4620\n",
      "Epoch: 49, Batch: 400, Loss: 0.5097\n",
      "Epoch: 49, Batch: 500, Loss: 0.4923\n",
      "Epoch: 49, Batch: 600, Loss: 0.5133\n",
      "Epoch: 50, Batch: 100, Loss: 0.7162\n",
      "Epoch: 50, Batch: 200, Loss: 0.6456\n",
      "Epoch: 50, Batch: 300, Loss: 0.6563\n",
      "Epoch: 50, Batch: 400, Loss: 0.5718\n",
      "Epoch: 50, Batch: 500, Loss: 0.5874\n",
      "Epoch: 50, Batch: 600, Loss: 0.5820\n",
      "Epoch: 51, Batch: 100, Loss: 0.4914\n",
      "Epoch: 51, Batch: 200, Loss: 0.4457\n",
      "Epoch: 51, Batch: 300, Loss: 0.4624\n",
      "Epoch: 51, Batch: 400, Loss: 0.4810\n",
      "Epoch: 51, Batch: 500, Loss: 0.4920\n",
      "Epoch: 51, Batch: 600, Loss: 0.4918\n",
      "Epoch: 52, Batch: 100, Loss: 0.4600\n",
      "Epoch: 52, Batch: 200, Loss: 0.4527\n",
      "Epoch: 52, Batch: 300, Loss: 0.4551\n",
      "Epoch: 52, Batch: 400, Loss: 0.4618\n",
      "Epoch: 52, Batch: 500, Loss: 0.4636\n",
      "Epoch: 52, Batch: 600, Loss: 0.5196\n",
      "Epoch: 53, Batch: 100, Loss: 0.5840\n",
      "Epoch: 53, Batch: 200, Loss: 0.5641\n",
      "Epoch: 53, Batch: 300, Loss: 0.5340\n",
      "Epoch: 53, Batch: 400, Loss: 0.5528\n",
      "Epoch: 53, Batch: 500, Loss: 0.9114\n",
      "Epoch: 53, Batch: 600, Loss: 0.6404\n",
      "Epoch: 54, Batch: 100, Loss: 0.5489\n",
      "Epoch: 54, Batch: 200, Loss: 0.6347\n",
      "Epoch: 54, Batch: 300, Loss: 0.4819\n",
      "Epoch: 54, Batch: 400, Loss: 0.4712\n",
      "Epoch: 54, Batch: 500, Loss: 0.4860\n",
      "Epoch: 54, Batch: 600, Loss: 0.4664\n",
      "Epoch: 55, Batch: 100, Loss: 0.4301\n",
      "Epoch: 55, Batch: 200, Loss: 0.3937\n",
      "Epoch: 55, Batch: 300, Loss: 0.4233\n",
      "Epoch: 55, Batch: 400, Loss: 0.4334\n",
      "Epoch: 55, Batch: 500, Loss: 0.4292\n",
      "Epoch: 55, Batch: 600, Loss: 0.4276\n",
      "Epoch: 56, Batch: 100, Loss: 0.4398\n",
      "Epoch: 56, Batch: 200, Loss: 0.4059\n",
      "Epoch: 56, Batch: 300, Loss: 0.4046\n",
      "Epoch: 56, Batch: 400, Loss: 0.4489\n",
      "Epoch: 56, Batch: 500, Loss: 0.4868\n",
      "Epoch: 56, Batch: 600, Loss: 0.5100\n",
      "Epoch: 57, Batch: 100, Loss: 0.4572\n",
      "Epoch: 57, Batch: 200, Loss: 0.4056\n",
      "Epoch: 57, Batch: 300, Loss: 0.4207\n",
      "Epoch: 57, Batch: 400, Loss: 0.4275\n",
      "Epoch: 57, Batch: 500, Loss: 0.4603\n",
      "Epoch: 57, Batch: 600, Loss: 0.4440\n",
      "Epoch: 58, Batch: 100, Loss: 0.3876\n",
      "Epoch: 58, Batch: 200, Loss: 0.3874\n",
      "Epoch: 58, Batch: 300, Loss: 0.3881\n",
      "Epoch: 58, Batch: 400, Loss: 0.4267\n",
      "Epoch: 58, Batch: 500, Loss: 0.4263\n",
      "Epoch: 58, Batch: 600, Loss: 0.4594\n",
      "Epoch: 59, Batch: 100, Loss: 1.1315\n",
      "Epoch: 59, Batch: 200, Loss: 0.6196\n",
      "Epoch: 59, Batch: 300, Loss: 0.5663\n",
      "Epoch: 59, Batch: 400, Loss: 0.4857\n",
      "Epoch: 59, Batch: 500, Loss: 0.4371\n",
      "Epoch: 59, Batch: 600, Loss: 0.4253\n",
      "Epoch: 60, Batch: 100, Loss: 0.4974\n",
      "Epoch: 60, Batch: 200, Loss: 0.4020\n",
      "Epoch: 60, Batch: 300, Loss: 0.3962\n",
      "Epoch: 60, Batch: 400, Loss: 0.3853\n",
      "Epoch: 60, Batch: 500, Loss: 0.3847\n",
      "Epoch: 60, Batch: 600, Loss: 0.4019\n",
      "Epoch: 61, Batch: 100, Loss: 0.7314\n",
      "Epoch: 61, Batch: 200, Loss: 0.4589\n",
      "Epoch: 61, Batch: 300, Loss: 0.4154\n",
      "Epoch: 61, Batch: 400, Loss: 0.4063\n",
      "Epoch: 61, Batch: 500, Loss: 0.4057\n",
      "Epoch: 61, Batch: 600, Loss: 0.3973\n",
      "Epoch: 62, Batch: 100, Loss: 0.3600\n",
      "Epoch: 62, Batch: 200, Loss: 0.3809\n",
      "Epoch: 62, Batch: 300, Loss: 0.3676\n",
      "Epoch: 62, Batch: 400, Loss: 0.3453\n",
      "Epoch: 62, Batch: 500, Loss: 0.3736\n",
      "Epoch: 62, Batch: 600, Loss: 0.3899\n",
      "Epoch: 63, Batch: 100, Loss: 0.5225\n",
      "Epoch: 63, Batch: 200, Loss: 0.4586\n",
      "Epoch: 63, Batch: 300, Loss: 0.4062\n",
      "Epoch: 63, Batch: 400, Loss: 0.3820\n",
      "Epoch: 63, Batch: 500, Loss: 0.4679\n",
      "Epoch: 63, Batch: 600, Loss: 0.4681\n",
      "Epoch: 64, Batch: 100, Loss: 0.4976\n",
      "Epoch: 64, Batch: 200, Loss: 0.4156\n",
      "Epoch: 64, Batch: 300, Loss: 0.4887\n",
      "Epoch: 64, Batch: 400, Loss: 0.4178\n",
      "Epoch: 64, Batch: 500, Loss: 0.3820\n",
      "Epoch: 64, Batch: 600, Loss: 0.4182\n",
      "Epoch: 65, Batch: 100, Loss: 0.3887\n",
      "Epoch: 65, Batch: 200, Loss: 0.3345\n",
      "Epoch: 65, Batch: 300, Loss: 0.3668\n",
      "Epoch: 65, Batch: 400, Loss: 0.3586\n",
      "Epoch: 65, Batch: 500, Loss: 0.3604\n",
      "Epoch: 65, Batch: 600, Loss: 0.3600\n",
      "Epoch: 66, Batch: 100, Loss: 0.3779\n",
      "Epoch: 66, Batch: 200, Loss: 0.3531\n",
      "Epoch: 66, Batch: 300, Loss: 0.3456\n",
      "Epoch: 66, Batch: 400, Loss: 0.3445\n",
      "Epoch: 66, Batch: 500, Loss: 0.3830\n",
      "Epoch: 66, Batch: 600, Loss: 0.4117\n",
      "Epoch: 67, Batch: 100, Loss: 0.8412\n",
      "Epoch: 67, Batch: 200, Loss: 0.6882\n",
      "Epoch: 67, Batch: 300, Loss: 0.5241\n",
      "Epoch: 67, Batch: 400, Loss: 0.4925\n",
      "Epoch: 67, Batch: 500, Loss: 0.4822\n",
      "Epoch: 67, Batch: 600, Loss: 0.4166\n",
      "Epoch: 68, Batch: 100, Loss: 0.3356\n",
      "Epoch: 68, Batch: 200, Loss: 0.3258\n",
      "Epoch: 68, Batch: 300, Loss: 0.3393\n",
      "Epoch: 68, Batch: 400, Loss: 0.3344\n",
      "Epoch: 68, Batch: 500, Loss: 0.3353\n",
      "Epoch: 68, Batch: 600, Loss: 0.3605\n",
      "Epoch: 69, Batch: 100, Loss: 0.3959\n",
      "Epoch: 69, Batch: 200, Loss: 0.3231\n",
      "Epoch: 69, Batch: 300, Loss: 0.3250\n",
      "Epoch: 69, Batch: 400, Loss: 0.3127\n",
      "Epoch: 69, Batch: 500, Loss: 0.3313\n",
      "Epoch: 69, Batch: 600, Loss: 0.3394\n",
      "Epoch: 70, Batch: 100, Loss: 0.3230\n",
      "Epoch: 70, Batch: 200, Loss: 0.3238\n",
      "Epoch: 70, Batch: 300, Loss: 0.3208\n",
      "Epoch: 70, Batch: 400, Loss: 0.3235\n",
      "Epoch: 70, Batch: 500, Loss: 0.3573\n",
      "Epoch: 70, Batch: 600, Loss: 0.3691\n",
      "Epoch: 71, Batch: 100, Loss: 0.3050\n",
      "Epoch: 71, Batch: 200, Loss: 0.3109\n",
      "Epoch: 71, Batch: 300, Loss: 0.3698\n",
      "Epoch: 71, Batch: 400, Loss: 0.3809\n",
      "Epoch: 71, Batch: 500, Loss: 0.4066\n",
      "Epoch: 71, Batch: 600, Loss: 0.4669\n",
      "Epoch: 72, Batch: 100, Loss: 0.5258\n",
      "Epoch: 72, Batch: 200, Loss: 0.3698\n",
      "Epoch: 72, Batch: 300, Loss: 0.3612\n",
      "Epoch: 72, Batch: 400, Loss: 0.3826\n",
      "Epoch: 72, Batch: 500, Loss: 0.3551\n",
      "Epoch: 72, Batch: 600, Loss: 0.3535\n",
      "Epoch: 73, Batch: 100, Loss: 0.3218\n",
      "Epoch: 73, Batch: 200, Loss: 0.3399\n",
      "Epoch: 73, Batch: 300, Loss: 0.3121\n",
      "Epoch: 73, Batch: 400, Loss: 0.3120\n",
      "Epoch: 73, Batch: 500, Loss: 0.3059\n",
      "Epoch: 73, Batch: 600, Loss: 0.3293\n",
      "Epoch: 74, Batch: 100, Loss: 0.4266\n",
      "Epoch: 74, Batch: 200, Loss: 0.4422\n",
      "Epoch: 74, Batch: 300, Loss: 0.3775\n",
      "Epoch: 74, Batch: 400, Loss: 0.3209\n",
      "Epoch: 74, Batch: 500, Loss: 0.3270\n",
      "Epoch: 74, Batch: 600, Loss: 0.3246\n",
      "Epoch: 75, Batch: 100, Loss: 0.3354\n",
      "Epoch: 75, Batch: 200, Loss: 0.3187\n",
      "Epoch: 75, Batch: 300, Loss: 0.3124\n",
      "Epoch: 75, Batch: 400, Loss: 0.2976\n",
      "Epoch: 75, Batch: 500, Loss: 0.3228\n",
      "Epoch: 75, Batch: 600, Loss: 0.3461\n",
      "Epoch: 76, Batch: 100, Loss: 0.3739\n",
      "Epoch: 76, Batch: 200, Loss: 0.4032\n",
      "Epoch: 76, Batch: 300, Loss: 0.3876\n",
      "Epoch: 76, Batch: 400, Loss: 0.3202\n",
      "Epoch: 76, Batch: 500, Loss: 0.3482\n",
      "Epoch: 76, Batch: 600, Loss: 0.3572\n",
      "Epoch: 77, Batch: 100, Loss: 0.7024\n",
      "Epoch: 77, Batch: 200, Loss: 0.4711\n",
      "Epoch: 77, Batch: 300, Loss: 0.4117\n",
      "Epoch: 77, Batch: 400, Loss: 0.3489\n",
      "Epoch: 77, Batch: 500, Loss: 0.3327\n",
      "Epoch: 77, Batch: 600, Loss: 0.3177\n",
      "Epoch: 78, Batch: 100, Loss: 0.6287\n",
      "Epoch: 78, Batch: 200, Loss: 0.4005\n",
      "Epoch: 78, Batch: 300, Loss: 0.3445\n",
      "Epoch: 78, Batch: 400, Loss: 0.3394\n",
      "Epoch: 78, Batch: 500, Loss: 0.3258\n",
      "Epoch: 78, Batch: 600, Loss: 0.3094\n",
      "Epoch: 79, Batch: 100, Loss: 0.3100\n",
      "Epoch: 79, Batch: 200, Loss: 0.2756\n",
      "Epoch: 79, Batch: 300, Loss: 0.2797\n",
      "Epoch: 79, Batch: 400, Loss: 0.2732\n",
      "Epoch: 79, Batch: 500, Loss: 0.2877\n",
      "Epoch: 79, Batch: 600, Loss: 0.2883\n",
      "Epoch: 80, Batch: 100, Loss: 0.2597\n",
      "Epoch: 80, Batch: 200, Loss: 0.2535\n",
      "Epoch: 80, Batch: 300, Loss: 0.2712\n",
      "Epoch: 80, Batch: 400, Loss: 0.2774\n",
      "Epoch: 80, Batch: 500, Loss: 0.2657\n",
      "Epoch: 80, Batch: 600, Loss: 0.2957\n",
      "Epoch: 81, Batch: 100, Loss: 0.3622\n",
      "Epoch: 81, Batch: 200, Loss: 0.2935\n",
      "Epoch: 81, Batch: 300, Loss: 0.2764\n",
      "Epoch: 81, Batch: 400, Loss: 0.2736\n",
      "Epoch: 81, Batch: 500, Loss: 0.2832\n",
      "Epoch: 81, Batch: 600, Loss: 0.2883\n",
      "Epoch: 82, Batch: 100, Loss: 0.2705\n",
      "Epoch: 82, Batch: 200, Loss: 0.3186\n",
      "Epoch: 82, Batch: 300, Loss: 0.3287\n",
      "Epoch: 82, Batch: 400, Loss: 0.3416\n",
      "Epoch: 82, Batch: 500, Loss: 0.4605\n",
      "Epoch: 82, Batch: 600, Loss: 0.6109\n",
      "Epoch: 83, Batch: 100, Loss: 0.5641\n",
      "Epoch: 83, Batch: 200, Loss: 0.4361\n",
      "Epoch: 83, Batch: 300, Loss: 0.3604\n",
      "Epoch: 83, Batch: 400, Loss: 0.3600\n",
      "Epoch: 83, Batch: 500, Loss: 0.3089\n",
      "Epoch: 83, Batch: 600, Loss: 0.3290\n",
      "Epoch: 84, Batch: 100, Loss: 0.3061\n",
      "Epoch: 84, Batch: 200, Loss: 0.2605\n",
      "Epoch: 84, Batch: 300, Loss: 0.2533\n",
      "Epoch: 84, Batch: 400, Loss: 0.2749\n",
      "Epoch: 84, Batch: 500, Loss: 0.2621\n",
      "Epoch: 84, Batch: 600, Loss: 0.2670\n",
      "Epoch: 85, Batch: 100, Loss: 0.2712\n",
      "Epoch: 85, Batch: 200, Loss: 0.2585\n",
      "Epoch: 85, Batch: 300, Loss: 0.2438\n",
      "Epoch: 85, Batch: 400, Loss: 0.2483\n",
      "Epoch: 85, Batch: 500, Loss: 0.2551\n",
      "Epoch: 85, Batch: 600, Loss: 0.2486\n",
      "Epoch: 86, Batch: 100, Loss: 0.2461\n",
      "Epoch: 86, Batch: 200, Loss: 0.2591\n",
      "Epoch: 86, Batch: 300, Loss: 0.2710\n",
      "Epoch: 86, Batch: 400, Loss: 0.2714\n",
      "Epoch: 86, Batch: 500, Loss: 0.2619\n",
      "Epoch: 86, Batch: 600, Loss: 0.2746\n",
      "Epoch: 87, Batch: 100, Loss: 1.1781\n",
      "Epoch: 87, Batch: 200, Loss: 0.5885\n",
      "Epoch: 87, Batch: 300, Loss: 0.4869\n",
      "Epoch: 87, Batch: 400, Loss: 0.4241\n",
      "Epoch: 87, Batch: 500, Loss: 0.3734\n",
      "Epoch: 87, Batch: 600, Loss: 0.3420\n",
      "Epoch: 88, Batch: 100, Loss: 0.3555\n",
      "Epoch: 88, Batch: 200, Loss: 0.2892\n",
      "Epoch: 88, Batch: 300, Loss: 0.2734\n",
      "Epoch: 88, Batch: 400, Loss: 0.2506\n",
      "Epoch: 88, Batch: 500, Loss: 0.2573\n",
      "Epoch: 88, Batch: 600, Loss: 0.2523\n",
      "Epoch: 89, Batch: 100, Loss: 0.2392\n",
      "Epoch: 89, Batch: 200, Loss: 0.2339\n",
      "Epoch: 89, Batch: 300, Loss: 0.2265\n",
      "Epoch: 89, Batch: 400, Loss: 0.2304\n",
      "Epoch: 89, Batch: 500, Loss: 0.2359\n",
      "Epoch: 89, Batch: 600, Loss: 0.2419\n",
      "Epoch: 90, Batch: 100, Loss: 0.9109\n",
      "Epoch: 90, Batch: 200, Loss: 0.4537\n",
      "Epoch: 90, Batch: 300, Loss: 0.3715\n",
      "Epoch: 90, Batch: 400, Loss: 0.3111\n",
      "Epoch: 90, Batch: 500, Loss: 0.2969\n",
      "Epoch: 90, Batch: 600, Loss: 0.2739\n",
      "Epoch: 91, Batch: 100, Loss: 0.3819\n",
      "Epoch: 91, Batch: 200, Loss: 0.2764\n",
      "Epoch: 91, Batch: 300, Loss: 0.2664\n",
      "Epoch: 91, Batch: 400, Loss: 0.2532\n",
      "Epoch: 91, Batch: 500, Loss: 0.2359\n",
      "Epoch: 91, Batch: 600, Loss: 0.2481\n",
      "Epoch: 92, Batch: 100, Loss: 0.2357\n",
      "Epoch: 92, Batch: 200, Loss: 0.2152\n",
      "Epoch: 92, Batch: 300, Loss: 0.2277\n",
      "Epoch: 92, Batch: 400, Loss: 0.2229\n",
      "Epoch: 92, Batch: 500, Loss: 0.2270\n",
      "Epoch: 92, Batch: 600, Loss: 0.2283\n",
      "Epoch: 93, Batch: 100, Loss: 0.5895\n",
      "Epoch: 93, Batch: 200, Loss: 0.3804\n",
      "Epoch: 93, Batch: 300, Loss: 0.3126\n",
      "Epoch: 93, Batch: 400, Loss: 0.2811\n",
      "Epoch: 93, Batch: 500, Loss: 0.2663\n",
      "Epoch: 93, Batch: 600, Loss: 0.2569\n",
      "Epoch: 94, Batch: 100, Loss: 0.2717\n",
      "Epoch: 94, Batch: 200, Loss: 0.2374\n",
      "Epoch: 94, Batch: 300, Loss: 0.2322\n",
      "Epoch: 94, Batch: 400, Loss: 0.2379\n",
      "Epoch: 94, Batch: 500, Loss: 0.2646\n",
      "Epoch: 94, Batch: 600, Loss: 0.2481\n",
      "Epoch: 95, Batch: 100, Loss: 0.2688\n",
      "Epoch: 95, Batch: 200, Loss: 0.2544\n",
      "Epoch: 95, Batch: 300, Loss: 0.2279\n",
      "Epoch: 95, Batch: 400, Loss: 0.2307\n",
      "Epoch: 95, Batch: 500, Loss: 0.2349\n",
      "Epoch: 95, Batch: 600, Loss: 0.2518\n",
      "Epoch: 96, Batch: 100, Loss: 0.2126\n",
      "Epoch: 96, Batch: 200, Loss: 0.2134\n",
      "Epoch: 96, Batch: 300, Loss: 0.2180\n",
      "Epoch: 96, Batch: 400, Loss: 0.2344\n",
      "Epoch: 96, Batch: 500, Loss: 0.2416\n",
      "Epoch: 96, Batch: 600, Loss: 0.2619\n",
      "Epoch: 97, Batch: 100, Loss: 0.3004\n",
      "Epoch: 97, Batch: 200, Loss: 0.5633\n",
      "Epoch: 97, Batch: 300, Loss: 0.5620\n",
      "Epoch: 97, Batch: 400, Loss: 0.4065\n",
      "Epoch: 97, Batch: 500, Loss: 0.3173\n",
      "Epoch: 97, Batch: 600, Loss: 0.2816\n",
      "Epoch: 98, Batch: 100, Loss: 0.3010\n",
      "Epoch: 98, Batch: 200, Loss: 0.2399\n",
      "Epoch: 98, Batch: 300, Loss: 0.2331\n",
      "Epoch: 98, Batch: 400, Loss: 0.2320\n",
      "Epoch: 98, Batch: 500, Loss: 0.2239\n",
      "Epoch: 98, Batch: 600, Loss: 0.2243\n",
      "Epoch: 99, Batch: 100, Loss: 0.2485\n",
      "Epoch: 99, Batch: 200, Loss: 0.2099\n",
      "Epoch: 99, Batch: 300, Loss: 0.2225\n",
      "Epoch: 99, Batch: 400, Loss: 0.2394\n",
      "Epoch: 99, Batch: 500, Loss: 0.2244\n",
      "Epoch: 99, Batch: 600, Loss: 0.2234\n",
      "Epoch: 100, Batch: 100, Loss: 0.3241\n",
      "Epoch: 100, Batch: 200, Loss: 0.2500\n",
      "Epoch: 100, Batch: 300, Loss: 0.2301\n",
      "Epoch: 100, Batch: 400, Loss: 0.3209\n",
      "Epoch: 100, Batch: 500, Loss: 0.4744\n",
      "Epoch: 100, Batch: 600, Loss: 0.3933\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have instantiated the UNetCIFAR model\n",
    "model = UNetCIFAR().to(device)  \n",
    "criterion = nn.MSELoss()  # Define the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)  # Define the optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        \n",
    "        # Reshape labels to match the expected format for CrossEntropyLoss\n",
    "        # labels = labels.unsqueeze(-2)  # Remove any single-dimensional entries\n",
    "        # labels = labels.long()  # Ensure labels are of type long (for indices)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.shape)\n",
    "\n",
    "        labels = labels.view(-1, 1, 1, 1)\n",
    "        labels = labels.repeat(1, 10, 32, 32)\n",
    "\n",
    "        loss = criterion(outputs.float(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 32, 32]' is invalid for input of size 655360",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pramesh/Dev/871_project/experiments/unet.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pramesh/Dev/871_project/experiments/unet.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         _, predicted \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pramesh/Dev/871_project/experiments/unet.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         total_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m  \u001b[39m# Total number of pixels\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pramesh/Dev/871_project/experiments/unet.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         correct_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m predicted\u001b[39m.\u001b[39meq(labels\u001b[39m.\u001b[39;49mview_as(predicted))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pramesh/Dev/871_project/experiments/unet.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Calculate metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pramesh/Dev/871_project/experiments/unet.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m train_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 32, 32]' is invalid for input of size 655360"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "total_val_loss = 0.0\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "        \n",
    "        labels = labels.view(-1, 1, 1, 1)\n",
    "        labels = labels.repeat(1, 10, 32, 32)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs.float(), labels)\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        # Compute validation accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_val += labels.size(0) * 32 * 32  # Total number of pixels\n",
    "        correct_val += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "\n",
    "# Calculate metrics\n",
    "train_loss = running_loss / len(train_loader)\n",
    "val_loss = total_val_loss / len(val_loader)\n",
    "val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "print(f\"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
